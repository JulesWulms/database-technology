In this section we will discuss the results and try to verify the claims made by the authors of our paper \cite{paper}. We will start by discussing our findings and then we will compare them to the findings of our paper \cite{paper}.

\subsection{Our Findings} \label{subsec:DissFindings}
From the graphs presented in Section~\ref{sec:Results} (Figure~\ref{fig:ResultsRandomOrder}, Figure~\ref{fig:ResultsRandomDensity2} and Figure~\ref{fig:ResultsRandomDensity3}) we can see that all approaches are exponential in performance. There are differences between the approaches however. The naive approach is faster then the other approaches in the long run. It starts slower, but in the long run it becomes faster. We can see this by looking at the steepness of the trend line. This also hints us that the naive approach is faster for the fixed density of 0.6, but the steepness of the trend line is higher. So in the long run it will become slower. The other approaches seem similar in performance. Those trend lines are almost on top of each other. 

We think this has to do with the combination of the density and the order. A higher density means that it is less likely to be 3-colour able. With a low density the naive approach benefits from the fact that there are a lot of nodes not connected to each other. Unconnected nodes to not lay constraints on each other. This means that when a unconnected node is added, the Cartesian product needs to be computed. Due to the way the naive approach works, the Cartesian product is already created and only needs to be filtered on. This could also explain why the other approaches are slower. The other approaches uses joins, which are not optimal to compute Cartesian products. They are optimized for joining tables. Which is beneficial in the higher density setting. More connected nodes, means more joins. 

Besides these explanation, we think there could be another explanation for the differences. We generate the queries and send them to PostgreSQL, which executes them. It is almost certain that PostgreSQL tries to optimize the queries, before he actually executes them. The naive approach is quite clear and simple and therefore it is quit likely that the optimizer finds a way to make it faster. It is likely that the optimizer notices that he can rewrite the query, with some smart order of joining for example. For the straightforward approach this could be true, but it already uses joins. The order of the joins is already specified, which would make it harder for the optimizer to optimize. For the early projection and reordering approach it is unlikely that the optimizer finds a way to optimize it. Those queries are complicated and are already fixed by the query generator. 

We can see in Figure~\ref{ResultsAugmented} and Figure~\ref{ResultsLadder} that the optimizations benefit from the structure in the graphs. The naive approach is the slowest of the four approaches. The steepness of the different approaches is similar and remains exponential. This has to do with the fact that more structure means that there are more nodes to join on and that there are also more nodes to project out during the computation. Which means that it can reduce the size of the intermediate results.

\subsection{Comparison of the Findings} \label{subsec:DissComparison}
Before we start the comparison, lets filer out all comparison that are actually applicable to our experiments. The conclusion of the authors of our paper \cite{paper} was that bucket elimination dominates. We cannot check this conclusion due a unstable implementation, as mentioned before. This means that we can only verify or reject a claim for the random graph type and the naive, straightforward, early projection and reordering approach. The claims we can verify can be found below: 

\begin{enumerate}
\item[\ref{claim:RunInc}] At first the running time increases as density increases, because of an increased number of joins.
\item[\ref{claim:SizeInter}] Eventually the size of intermediate results becomes small or empty, and additional joins have little effect on overall running time.
\item[\ref{claim:IncImprov}] At low density each optimization method improves upon the previous. For denser instances, optimizations using early projection lose their effectiveness.
\item[\ref{claim:ExpoInc}] All methods show exponential increase in running, when order is increased. (This is shown by a linear slope in log-scale.)
\item[\ref{claim:AugLadReordering}] For ladder instances, the heuristic for reordering is not only unable to find a better order, but actually finds a worse one.
\item[\ref{claim:AugLadSimilar}] Furthermore, ladder instances give results very similar to augmented path instances.
\item[\ref{claim:AugLadMoreDiff}] Augmented ladder instances shows even more differences between optimization methods.
\end{enumerate}

The general claims about the increase in performance when density increases (\ref{claim:RunInc} and the exponential increase in performance when order increases \ref{claim:ExpoInc} are verified. This can be seen in all graphs (Figure~\ref{fig:ResultsRandomOrder}, Figure~\ref{fig:ResultsRandomDensity2} and Figure~\ref{fig:ResultsRandomDensity3}) as all trend lines are increasing. 

The next claim we can verify, is the claim about the low density (\ref{claim:IncImprov}). We can say that we cannot verify this claim, as each optimization performs less than the naive approach. With the results we could even say that we reject the claim. As mentioned before this could be caused by the optimizer of PostgreSQL and by the number of joins that compute a Cartesian product.

The claim about the intermediate results (\ref{claim:SizeInter}) is harder to verify. We can implicitly verify it. We noticed that the queries that uses joins are performing worse than those that do not use joins (the naive approach). We know that joins are better at performing joins. There are two paths we can take to verify this claim. The first is by using the variables that are used in the join and the second is by using the intermediate sets. When the variables that are used for the join are not common, the Cartesian product is computed. If the intermediate results would be smaller or empty, this operation is more efficient. Hence the performance would be better. This is not the case as it performs worse than the naive approach, even in the long run. Now lets take the other path. If the intermediate results becomes smaller or empty, there less common variables to join on. Which would imply that there are more Cartesian products computed and thereby the performance becomes worse. Which can be seen in the results. Both the paths argue that this claim can be rejected.

The claims about the different types of graphs (Claim~\ref{claim:AugLadReordering}, \ref{claim:AugLadSimilar} and \ref{claim:AugLadMoreDiff}) can be checked by looking at Figure~\ref{fig:ResultsAugmented} and \ref{fig:ResultsLadder}. We can see that Claim~\ref{claim:AugLadSimilar} holds and that Claim~\ref{claim:AugLadMoreDiff} does not hold. The graphs look the same, but the more complex approaches remain around the same. They are now faster as the naive approach, but that is not claimed by the paper \cite{paper}.   Claim~\ref{claim:AugLadReordering} is an interesting one. If we look to the graphs and judge strictly it means that is holds, but the difference is so small that it could be caused by other factors than the order. We can say that the authors are on the right track, but for a good validation of this claim, we need to rule out other possibilities. These possibilities can include the PostgreSQL engine and the ``lucky'' order for the early projection approach. 